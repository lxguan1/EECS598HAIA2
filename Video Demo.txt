The labeling task that I decided to do was 2D visual question answering, more specifically the Balanced Binary Abstract Scenes dataset from the visual question answering datasets. The goal of these datasets was that given some relatively abstract images, some relatively abstract questions would be asked about the contents of the images, and each of the questions had a yes or no answer. So the annotations I will be collecting would be these yes or no answers by crowdworkers on these images. Specifically I am allowing for repeats, such that each image can have the opinions of up to 5 people, and either majority voting or averaging can be done on their responses. As for the interface itself, let us look at the HIT here on Amazon Mechanical Turk which I published. I chose to do the survey code style, which means that my webpage will give the user a code after all of the questions are answered that they can put into this survey code textbox. Let's look at the webpage itself. You can see that there is a more in depth explanation of the task, and each task is 5 questions. Each question has an image paired with a yes or no question, which the user can select. The user must finish all of the questions before they are given the survey code, and are unable to change their responses after they submit. I estimated that the questions should take at most 30 seconds each, so the final rate for one task was 0.50 cents. 